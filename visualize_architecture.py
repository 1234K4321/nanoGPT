"""
Visualization of synchronization points in tensor parallel model.
This script generates a simple diagram showing where all-reduces occur.
"""

def print_architecture_diagram():
    """Print ASCII diagram of tensor parallel architecture."""
    
    print("=" * 80)
    print("TENSOR PARALLEL GPT ARCHITECTURE (2-way parallelism)")
    print("=" * 80)
    print()
    
    print("Input: Token IDs")
    print("  ↓")
    print("Token Embeddings + Position Embeddings")
    print("  ↓")
    print()
    
    n_layers = 12  # Default nanoGPT config
    
    for layer_idx in range(n_layers):
        print(f"┌─── Transformer Block {layer_idx + 1} ───────────────────────────────────────┐")
        print("│                                                                    │")
        print("│  Layer Norm                                                        │")
        print("│    ↓                                                               │")
        print("│  ┌──────────────── ATTENTION (Tensor Parallel) ─────────────────┐ │")
        print("│  │                                                               │ │")
        print("│  │  GPU 1:                          GPU 2:                      │ │")
        print("│  │    Q₁, K₁, V₁ = X·Wqkv₁           Q₂, K₂, V₂ = X·Wqkv₂      │ │")
        print("│  │    (First half of heads)          (Second half of heads)     │ │")
        print("│  │          ↓                               ↓                   │ │")
        print("│  │    Head₁ = Attn(Q₁,K₁,V₁)         Head₂ = Attn(Q₂,K₂,V₂)   │ │")
        print("│  │          ↓                               ↓                   │ │")
        print("│  │    Z₁ = Head₁·Wo₁                 Z₂ = Head₂·Wo₂            │ │")
        print("│  │          ↓                               ↓                   │ │")
        print("│  │          └───────────────┬───────────────┘                   │ │")
        print(f"│  │                    ALL-REDUCE #{layer_idx * 2 + 1:<3}                         │ │")
        print("│  │                          ↓                                    │ │")
        print("│  │                      Z = Z₁ + Z₂                              │ │")
        print("│  └───────────────────────────────────────────────────────────────┘ │")
        print("│    ↓                                                               │")
        print("│  Residual Connection                                               │")
        print("│    ↓                                                               │")
        print("│  Layer Norm                                                        │")
        print("│    ↓                                                               │")
        print("│  ┌────────────────── MLP (Tensor Parallel) ────────────────────┐ │")
        print("│  │                                                               │ │")
        print("│  │  GPU 1:                          GPU 2:                      │ │")
        print("│  │    Y₁ = GeLU(X·A₁)                Y₂ = GeLU(X·A₂)            │ │")
        print("│  │    (First half of hidden dim)     (Second half)              │ │")
        print("│  │          ↓                               ↓                   │ │")
        print("│  │    Z₁ = Y₁·B₁                      Z₂ = Y₂·B₂                │ │")
        print("│  │          ↓                               ↓                   │ │")
        print("│  │          └───────────────┬───────────────┘                   │ │")
        print(f"│  │                    ALL-REDUCE #{layer_idx * 2 + 2:<3}                         │ │")
        print("│  │                          ↓                                    │ │")
        print("│  │                      Z = Z₁ + Z₂                              │ │")
        print("│  └───────────────────────────────────────────────────────────────┘ │")
        print("│    ↓                                                               │")
        print("│  Residual Connection                                               │")
        print("└────────────────────────────────────────────────────────────────────┘")
        print("  ↓")
        print()
    
    print("Final Layer Norm")
    print("  ↓")
    print("Language Model Head")
    print("  ↓")
    print("Output: Logits over vocabulary")
    print()
    
    total_syncs = n_layers * 2
    print("=" * 80)
    print(f"SUMMARY: {total_syncs} synchronization points (all-reduces) per forward pass")
    print(f"  • {n_layers} in attention layers")
    print(f"  • {n_layers} in MLP layers")
    print("=" * 80)
    print()


def print_comparison_table():
    """Print comparison of different parallelism strategies."""
    
    print("=" * 80)
    print("COMPARISON OF PARALLELISM STRATEGIES")
    print("=" * 80)
    print()
    
    print("For a 12-layer GPT model with 256 sequence length:")
    print()
    print("┌─────────────────────┬──────────────────┬─────────────────────────────┐")
    print("│ Strategy            │ Sync Points      │ Description                 │")
    print("├─────────────────────┼──────────────────┼─────────────────────────────┤")
    print("│ Data Parallel       │ 1 (per step)     │ Gradients sync only         │")
    print("│                     │                  │ (training only)             │")
    print("├─────────────────────┼──────────────────┼─────────────────────────────┤")
    print("│ Pipeline Parallel   │ ~256 per layer   │ Pass activations between    │")
    print("│ (naive)             │ = ~3,072 total   │ stages for each token       │")
    print("├─────────────────────┼──────────────────┼─────────────────────────────┤")
    print("│ Tensor Parallel     │ 2 per layer      │ All-reduce after attention  │")
    print("│ (this impl)         │ = 24 total       │ and MLP in each layer       │")
    print("├─────────────────────┼──────────────────┼─────────────────────────────┤")
    print("│ Hybrid              │ Varies           │ Combine multiple strategies │")
    print("│ (TP + PP + DP)      │ (optimized)      │ for best performance        │")
    print("└─────────────────────┴──────────────────┴─────────────────────────────┘")
    print()
    
    print("Latency Impact (approximate, per token):")
    print("  • Data Parallel:     No inference overhead")
    print("  • Pipeline Parallel: Very High (~100-1000ms for large models)")
    print("  • Tensor Parallel:   Moderate (~1-10ms depending on network)")
    print("  • Hybrid:            Low to Moderate (optimized for specific hardware)")
    print()
    print("=" * 80)
    print()


def print_weight_splitting_diagram():
    """Print diagram showing how weights are split."""
    
    print("=" * 80)
    print("WEIGHT SPLITTING STRATEGY")
    print("=" * 80)
    print()
    
    print("MLP LAYER:")
    print()
    print("  Original:  X[B,T,E] → Linear(E, 4E) → GeLU → Linear(4E, E) → Z[B,T,E]")
    print()
    print("  Tensor Parallel (2-way):")
    print()
    print("             GPU 1:                      GPU 2:")
    print("             ┌─────────────────┐         ┌─────────────────┐")
    print("  X[B,T,E] → │ Linear(E, 2E)   │         │ Linear(E, 2E)   │")
    print("             │      ↓          │         │      ↓          │")
    print("             │    GeLU         │         │    GeLU         │")
    print("             │      ↓          │         │      ↓          │")
    print("             │ Linear(2E, E)   │         │ Linear(2E, E)   │")
    print("             │      ↓          │         │      ↓          │")
    print("             │  Z₁[B,T,E]      │         │  Z₂[B,T,E]      │")
    print("             └─────────────────┘         └─────────────────┘")
    print("                      │                           │")
    print("                      └───────── ALL-REDUCE ──────┘")
    print("                                  ↓")
    print("                            Z = Z₁ + Z₂")
    print()
    
    print("ATTENTION LAYER:")
    print()
    print("  Original:  X[B,T,E] → Q,K,V[B,T,E] → Attention → Linear(E,E) → Z[B,T,E]")
    print("             (with H heads, each of size E/H)")
    print()
    print("  Tensor Parallel (2-way):")
    print()
    print("             GPU 1 (H/2 heads):          GPU 2 (H/2 heads):")
    print("             ┌─────────────────┐         ┌─────────────────┐")
    print("  X[B,T,E] → │ Q,K,V (E/2)     │         │ Q,K,V (E/2)     │")
    print("             │      ↓          │         │      ↓          │")
    print("             │  Attention      │         │  Attention      │")
    print("             │  (first H/2)    │         │  (second H/2)   │")
    print("             │      ↓          │         │      ↓          │")
    print("             │ Linear(E/2, E)  │         │ Linear(E/2, E)  │")
    print("             │      ↓          │         │      ↓          │")
    print("             │  Z₁[B,T,E]      │         │  Z₂[B,T,E]      │")
    print("             └─────────────────┘         └─────────────────┘")
    print("                      │                           │")
    print("                      └───────── ALL-REDUCE ──────┘")
    print("                                  ↓")
    print("                            Z = Z₁ + Z₂")
    print()
    print("=" * 80)
    print()


def main():
    """Main function to print all visualizations."""
    
    print()
    print_architecture_diagram()
    print_comparison_table()
    print_weight_splitting_diagram()
    
    print("=" * 80)
    print("KEY INSIGHTS")
    print("=" * 80)
    print()
    print("1. REDUCED COMMUNICATION:")
    print("   Tensor parallelism requires only O(L) synchronization points where L is")
    print("   the number of layers, compared to O(L×T) for naive pipeline parallelism")
    print("   where T is the sequence length.")
    print()
    print("2. WORK DISTRIBUTION:")
    print("   Each GPU processes half the computation independently before synchronizing,")
    print("   enabling true parallel speedup for single-request inference.")
    print()
    print("3. MEMORY EFFICIENCY:")
    print("   Weights are distributed across GPUs, reducing per-GPU memory requirements")
    print("   while maintaining full model capacity.")
    print()
    print("4. SCALABILITY:")
    print("   For N GPUs, each synchronization becomes an all-reduce over N nodes,")
    print("   with logarithmic communication complexity: O(log N).")
    print()
    print("5. TRADE-OFFS:")
    print("   - Requires high-bandwidth interconnect (e.g., NVLink, InfiniBand)")
    print("   - Communication overhead grows with number of GPUs")
    print("   - Best suited for large models where computation >> communication")
    print()
    print("=" * 80)
    print()


if __name__ == "__main__":
    main()
